{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "vanilla_VAE.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TJ2v8tAFfjL"
      },
      "source": [
        "# Vanilla implementation of VAE\r\n",
        "\r\n",
        "This notebook is a vanilla implementation of VAE on the MNIST dataset. A VAE is a probabilistic take on the autoencoder, a model which takes high dimensional input data compress it into a smaller representation. Unlike a traditional autoencoder, which maps the input onto a latent vector, a VAE maps the input data into the parameters of a probability distribution, such as the mean and variance of a Gaussian. This approach produces a continuous, structured latent space, which is useful for image generation.\r\n",
        "\r\n",
        "[AutoEncoder Structure](https://www.tensorflow.org/tutorials/generative/cvae)\r\n",
        "\r\n",
        "[Loss functions and sampling](https://towardsdatascience.com/6-different-ways-of-implementing-vae-with-tensorflow-2-and-tensorflow-probability-9fe34a8ab981)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueAsYhY8q8P5"
      },
      "source": [
        "import numpy as np\r\n",
        "from tqdm import tqdm \r\n",
        "from IPython import display\r\n",
        "from tensorflow import keras\r\n",
        "import glob\r\n",
        "import imageio\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import PIL\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_probability as tfp\r\n",
        "import time\r\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk8uy9o54TpR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c67dfaec-bb95-446d-b568-8eae3cfbec5b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QNluonsFA18"
      },
      "source": [
        "tfd = tfp.distributions\r\n",
        "tfpl = tfp.layers\r\n",
        "tfk = tf.keras\r\n",
        "tfkl = tf.keras.layers"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0dIuA3UrFZJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35ba8e57-e353-43f1-d833-b8489d14e80b"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\r\n",
        "if device_name != '/device:GPU:0':\r\n",
        "  raise SystemError('GPU device not found')\r\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msjka61MrOOq"
      },
      "source": [
        "## Load the MNIST dataset\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR7NTwe1rN3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64d67c1b-733f-4363-f90c-1e724f385d1d"
      },
      "source": [
        "(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ewQ4mHBGgg7"
      },
      "source": [
        "train_size = 60000\r\n",
        "batch_size = 100\r\n",
        "test_size = 10000\r\n",
        "input_shape = (28, 28, 1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDNWqZe8FWer"
      },
      "source": [
        "def preprocess_images(images):\r\n",
        "  images = images.reshape((images.shape[0], 28, 28, 1)) / 255.\r\n",
        "  return np.where(images > .5, 1.0, 0.0).astype('float32')\r\n",
        "\r\n",
        "train_images = preprocess_images(train_images)\r\n",
        "test_images = preprocess_images(test_images)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCmn-6FSWcNO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c0b5933-5de3-4dc0-98ad-aa49e4266b47"
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roVrdGcqGlFg"
      },
      "source": [
        "### Batch and shuffle the data with tf.data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4YxuZB7Gjtg"
      },
      "source": [
        "train_dataset = (tf.data.Dataset.from_tensor_slices(train_images)\r\n",
        "                 .shuffle(train_size).batch(batch_size))\r\n",
        "test_dataset = (tf.data.Dataset.from_tensor_slices(test_images)\r\n",
        "                .shuffle(test_size).batch(batch_size))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xqq4XYffG-B0"
      },
      "source": [
        "# Encoder and decoder networks\r\n",
        "\r\n",
        "### Reparameterization trick\r\n",
        "\r\n",
        "To generate a sample  for the decoder during training, we can sample from the latent distribution defined by the parameters outputted by the encoder, given an input observation . However, this sampling operation creates a bottleneck because backpropagation cannot flow through a random node.\r\n",
        "\r\n",
        "To address this, we use a reparameterization trick. In our example, we approximate  using the decoder parameters and another parameter  as follows:\r\n",
        "\r\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPAAAABDCAYAAACiC4hJAAAKDElEQVR4Ae2biVMVVxbG86fMVGXivsYxo9EsE50QUSNG3NcYBIlxj0HFQsBM1bBGYcSYSakgaAhmVFBjknLXiBXBzKRAymhUGMWxBuNSlhbomfqO6a73+i08eW3bLd+t6urXt2+fe/p3z3f73tv9nhMmEiABzxJ4zrOe03ESIAGhgBkEJOBhAhSwhxuPrpMABcwYIAEPE6CAPdx4dJ0EKGDGAAl4mAAF7OHGo+skQAEzBkjAwwQoYA83Hl0nAQqYMUACHiZAAXu48eg6CVDAjAES8DABCtjDjUfXSYACZgyQgIcJUMAebjy6TgIUMGOABDxMgAL2cOPRdRKggBkDJOBhAhSwhxuPrpMABcwYIAEPE6CAPdx4dJ0EKGDGAAl4mAAF7OHGo+skQAF7LAYePHjgCY8fPnwora2t0tbWJvAZG/KY7CVAAdvL84lagwhiY0erMJ5oRVEYh4+lpWWyZMmHMnLUGOnWvZcMGjxUJk+ZJitTV8kPP5ymkKPga72UArYScfExxPG73z+vTzU3unnnzh1ZuGixjIl7R0pKtkrtmTP65L18+bKcPFktH6WsUP937tqt+W68B6/5RAF7qMXaXCzgq1evqjiXLUsJKU4MoWtrz8i4cRNk9ntzQpbzUJM8dVddK2DMn8q2bZfduyv9njjHjh2X3Nz8pw7O6gCejhge7tq123pKA7Wp6T9RB2xbm/1PYIgK81RwNbajR4+Zv5F34sT3Affkm4HrJ06aom0FDu0llMlc87GkZ2Q+9nDamFtfu/Zf08fOPLd2pYCbm5tl2vSZ2qNjyDhl6nSpq6uTCxcu6Hyqvv5sezGigYGG7ejWbgWWAhU7vlJ/ExISA4IyJzdPz+Xlf2K56vEOIRQ7h9C3b9+WtNXpMmToqyZr2LduiUnJIR2FGNcVFErCnKTH6qBwLyNHvi2HDh0Oadt6Atds/6JcZs6abfo48KXB8ssvF61FO82x6wSMgJibPE+WLP1QEPhYDHkzZoT07tNfGy3YE87aWg0NDTJh4uSotkuXLlvNhjxGJ4EFGgT+xs/+4VcO54zOqL0nmd+FQQ7sFPDFi5eUT4+efWT8hEnKGoIyxItRDvhv2rxFO84g7mgWfIqJiZXz588HLWJ0oMFOpqamSVpaekCHF6wsOhsMz+Hfpxs/09EO6saoBHV01uQ6AaMxUlJWmL05ju/fv6/D5r9lZZv54RoM7WkdBhrDw0j3kQwFDR9QNi5unAZX9alTRrbuf/31przQpZs8/4euflMBv0IRHtgl4JaWFnlrxEjp13+A/Pjjv0wB4D7Q8UEkdXX16lV74mhsbJQRsaNNG7630tR0RRIT56rwgnVe5V9WyIyZ77bbpleuXJFRo8aoX3l50Y1ifP17Fn67TsBWqAiqtWsLJP+Tta59fWIIq0+f/nLz5k2/Wzhw4KAG3tRpM4IGuV9hEZ0qhOpkjhw5qrawD1Wmvv6R8Kx2jWPwXL06Q+1UVe0xss19ds6j4X5p2TYzL9yPvXv3ybwPFgQUwRrGggWLtB50CBs2bAwQ6tmzDTp8B79QCR1Ibl6+2nl3doIUFX0qZWXbBL4fOHgoIqahbD8L+a4WcGtrmyQlJQt6agSeW9ORo4+EBV+tT6z09EwNvsLC9RG5n5WdE3boDzGEmx5AgOHS4cNH1B/Me4MxNebrW7YUhzNjnsOUISPzY/PY+AFR+s6tFy9eGlAfOju8Jw4nYPg4+u04syPA/ftuWP3uzMm1Akajdu3WQ6qrTwU0fCQNhoaPZoukDpSBYLOycjSo/rlzl99lODd2bLyeO326xu9cqANcE2rDUw3Bi32oMsgPl8q2f6E2Vq1KCygGXpgPow7rVCCg8G8ZNTU1+pGG9TxeeWXn5KotCPnb776zFtFpDj72QL2hEuKgb78BagfvknHsu4W6rrPku1LAaKDhw2Nk/zff+j3REJvB5lLWxsIqNoIQ884uXXsIFmowvO3/4kAZ+NIgXcke+srr8vqfh8kbw97URTJ84YSefuw78RI/fpJgkSeShOCLHz9R68Ow1jfdu3dPuvforfXjniAuzM07mmAD94V9R1NB4Xq1sTnIExadDOy/+tobcuvWrYiqwD2+/PIrQUWI+8XrM3Q4wdKmTZtl0aIlfm1sLYd7NQRsnZ5Yy3bGY9cJGF/z4CmAd8C+gYpgwAIGho/hemyjEXFtNFskdaAurI6+0KW7Bn5z8zWjet1/f7Ja842PFvCqCfcWqW0/YyJ6P9EKeGtpmfp0/PgJq3lzzrp//zcB50Jl4F4SEpJ0ThqqTLB8XDd9xiyd3wY7b+ShnDGEPnfunJHN/W8EXCXgGzdu6OoognRM3Dh90Y/3fnv27hMM+fA0+/nn4K8rnlaLYmgIf7FVVlaZbtTVNcjwv7yl+ZhXIqHzgYg7mtAhRStgrDp3695TCgoKTTfQOe74aqfanvOY73NhBAtn6MR8O1zTeJAfEOWW4hIZETsqos5sfdEG9Q2fZ/om1Hf8RGBH5FvmWf/tGgGjUVenZ8jyFaly/fp12ba9XAb88U/acIZAiotLImpwpxoNgZ+V/WieBx8/mL9QKquqdJiMd9kVFTvU/2nTZ+lXSkOGvhaV/3YIGJyxqovOJD1jjezd97W+ooP/KctXdsg/cMCqNV5N4RVVuISyJSWlyqWxsSlcUfMcfF720XK9BothmKoUF2/VqUty8jyzXGf84SoBI4gwJDXSv3/6SVasTNVVTgyf0PhuSggsfCUGvzEk/fv6Ihk/YbIkzX3fnENjODp//kJ9tx1qLhjpPdkhYNQFv8vLK/TPBe8lJEpR0QY5ePBwVHxhEwuOffu9KJ9/vlnr8G0v/Mb9L126TOLjJ+rcONL7RjksitXU1kpOXr5OQ9DZYMSDejtzco2AwzWCbyCEK+f0uf+1tEjPXn0Fn/NhgQV+Pklf7RKwwcluf2Hv7t27+pfHQYOHyNSpMyRzzV8FnQS+pMOUYt26woiH2oafvnvUAdF2duEaTDwhYMNZt+0rq/bo0xeLMU4FFIa/T7KTsIMxWKCzwZ878Nkj/j6I75WR53bf7bh/J21QwB2kjUDE8B7DZ8zHmEjgaRCggDtIHU+TYcNjpFfvfhLJv6M6WA0vI4GwBCjgsHjCn8SKebBPBMNfxbMkYB8BCtg+lrREAo4ToIAdR84KScA+AhSwfSxpiQQcJ0ABO46cFZKAfQQoYPtY0hIJOE6AAnYcOSskAfsIUMD2saQlEnCcAAXsOHJWSAL2EaCA7WNJSyTgOAEK2HHkrJAE7CNAAdvHkpZIwHECFLDjyFkhCdhHgAK2jyUtkYDjBChgx5GzQhKwjwAFbB9LWiIBxwlQwI4jZ4UkYB8BCtg+lrREAo4ToIAdR84KScA+AhSwfSxpiQQcJ0ABO46cFZKAfQQoYPtY0hIJOE6AAnYcOSskAfsI/B8jqy8XBHOsoAAAAABJRU5ErkJggg==)\r\n",
        "\r\n",
        "where  and  represent the mean and standard deviation of a Gaussian distribution respectively. They can be derived from the decoder output. The  can be thought of as a random noise used to maintain stochasticity of . We generate  from a standard normal distribution.\r\n",
        "\r\n",
        "The latent variable  is now generated by a function of ,  and , which would enable the model to backpropagate gradients in the encoder through  and  respectively, while maintaining stochasticity through ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgSV25TDHIGt"
      },
      "source": [
        "class VAE(tf.keras.Model):\r\n",
        "    \"\"\"Variational autoencoder.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, latent_dim, input_shape):\r\n",
        "        super(VAE, self).__init__()\r\n",
        "        self.latent_dim = latent_dim\r\n",
        "        self.encoder = tfk.Sequential(\r\n",
        "            [\r\n",
        "                tfkl.InputLayer(input_shape=input_shape),\r\n",
        "                tfkl.Flatten(),\r\n",
        "                tfkl.Dense(300),\r\n",
        "                # No activation\r\n",
        "                tfkl.Dense(300)\r\n",
        "            ]\r\n",
        "        )\r\n",
        "\r\n",
        "        self.decoder = tfk.Sequential(\r\n",
        "            [\r\n",
        "                tfkl.InputLayer(input_shape=(latent_dim,)),\r\n",
        "                tfkl.Dense(300),\r\n",
        "                # No activation\r\n",
        "                tfkl.Dense(300),\r\n",
        "            ]\r\n",
        "        )\r\n",
        "\r\n",
        "    @tf.function\r\n",
        "    def sample(self, eps=None, N=100):\r\n",
        "        if eps is None:\r\n",
        "            eps = tf.random.normal(shape=(N, self.latent_dim))\r\n",
        "        return self.decode(eps, apply_sigmoid=True)\r\n",
        "\r\n",
        "    def encode(self, x):\r\n",
        "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\r\n",
        "        return mean, logvar\r\n",
        "\r\n",
        "    def reparameterize(self, mean, logvar):\r\n",
        "        eps = tf.random.normal(shape=mean.shape)\r\n",
        "        return eps * tf.math.exp(logvar * .5) + mean\r\n",
        "\r\n",
        "    def decode(self, z, apply_sigmoid=False):\r\n",
        "        print(z.shape)\r\n",
        "        logits = self.decoder(z)\r\n",
        "        if apply_sigmoid:\r\n",
        "            probs = tf.sigmoid(logits)\r\n",
        "            return probs\r\n",
        "        return logits\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o63fJNEPXbRd"
      },
      "source": [
        "#### Define the loss function and the optimizer\r\n",
        "we will be using a Bernoulli distributed MLP as decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drcKRl5NXaeM"
      },
      "source": [
        "optimizer = tfk.optimizers.Adam(1e-4)\r\n",
        "\r\n",
        "def normal_log_pdf(sample, mean, logvar, raxis=1):\r\n",
        "  log2pi = tf.math.log(2. * np.pi)\r\n",
        "  return tf.reduce_sum(\r\n",
        "      -.5 * ((sample - mean) ** 2. * tf.math.exp(-logvar) + logvar + log2pi),\r\n",
        "      axis=raxis)\r\n",
        "\r\n",
        "# vae cost function as negative ELBO\r\n",
        "def vae_cost(x_true, model, analytic_kl=True, kl_weight=1):\r\n",
        "    # forward pass\r\n",
        "    mean, logvar = model.encode(x_true)\r\n",
        "    # z ~ q(z | x)\r\n",
        "    z_sample = model.reparameterize(mean, logvar)\r\n",
        "    x_recons_logits = model.decoder(z_sample)\r\n",
        "    # compute cross entropy loss for each dimension of every datapoint\r\n",
        "    raw_cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=x_true,\r\n",
        "                        logits=x_recons_logits)  # shape=(batch_size, 28, 28, 1)\r\n",
        "    # compute cross entropy loss for all instances in mini-batch; shape=(batch_size,)\r\n",
        "    neg_log_likelihood = tf.math.reduce_sum(raw_cross_entropy, axis=[1, 2, 3])\r\n",
        "    # compute reverse KL divergence, either analytically \r\n",
        "    # or through MC approximation with one sample\r\n",
        "    if analytic_kl:\r\n",
        "        kl_divergence = - 0.5 * tf.math.reduce_sum(\r\n",
        "            1 + tf.math.log(tf.math.exp(logvar) - tf.math.square(mean) - tf.math.exp(logvar)),\r\n",
        "            axis=1 #shape=(batch_size,)\r\n",
        "        )\r\n",
        "    else:\r\n",
        "        logpz = normal_log_pdf(z_sample, 0., 0.)  # shape=(batch_size,)\r\n",
        "        logqz_x = normal_log_pdf(z_sample, mean, logvar)  # shape=(batch_size,)\r\n",
        "        kl_divergence = logqz_x - logpz\r\n",
        "    elbo = tf.math.reduce_mean(-kl_weight * kl_divergence - neg_log_likelihood)  # shape=()\r\n",
        "    return -elbo\r\n",
        "\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def train_step(x_true, model, optimizer, analytic_kl=True, kl_weight=1):\r\n",
        "    \"\"\"Executes one training step and returns the loss.\r\n",
        "\r\n",
        "    This function computes the loss and gradients, and uses the latter to\r\n",
        "    update the model's parameters.\r\n",
        "    \"\"\"\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "        cost_mini_batch = vae_cost(x_true, model, analytic_kl, kl_weight)\r\n",
        "    gradients = tape.gradient(cost_mini_batch, model.trainable_variables)\r\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APRrnEdyy5Af"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcWkFbpeyzk6"
      },
      "source": [
        "epochs = 10\r\n",
        "# set the dimensionality of the latent space to a plane for visualization later\r\n",
        "latent_dim = 2\r\n",
        "num_examples_to_generate = 16\r\n",
        "\r\n",
        "# keeping the random vector constant for generation (prediction) so\r\n",
        "# it will be easier to see the improvement.\r\n",
        "random_vector_for_generation = tf.random.normal(\r\n",
        "    shape=[num_examples_to_generate, latent_dim])\r\n",
        "model = VAE(latent_dim, input_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHgTPZEH3-1H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3457025d-aa7a-443f-be9c-53eaa13a0a59"
      },
      "source": [
        "keras.utils.plot_model(model, \"my_model.png\", show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFMAAAA8CAYAAAAaEIaPAAAABmJLR0QA/wD/AP+gvaeTAAAEB0lEQVR4nO2cv0tybRzGL6U4GpFRCAZ5iFwq+jE0FAZR/QVFWUFTEEEOQj9QiKhoCFo6DVFLQ0MQao0NQUsFibUUERhOmRQEJQZakfZ9h5fngE/5vtTztR7r/sAZvC+P5/LDrefmRtQQEUHAgVf71Q2+E0ImI0ImI0ImIzm/D/h8PszPz39Fl6zC6/W+Gns1My8vL7GxsfEphbKRcDic1s+rmfmLt8wLAI/Hg56enjcz8Z3JiJDJiJDJiJDJiJDJiJDJiJDJiJDJiJDJiJDJiJDJiJDJiJDJiJDJyI+Q+fLyAkVRYLVaM3qdby8zGAyiubkZIyMjiMfjGb1W2s3h78DJyQlmZmYwNDSEWCyGTP9E4FvPzLq6OmxubqKvrw+SJGX8emwyKysrodFooNVqUV9fr36knE4nDAYDdDodVldXAQD7+/uoqqpSx2tqarC9va2+VjKZxOTkJGRZhl6vR21tLdxuN1fVzEG/4Xa76Y3h/yWRSFBZWRnJskyJRCIlGx4eJkVR1Mder5emp6fp7u6Obm9vqbGxkYqLi9V8bGyMJEmijY0NikQiND4+Tlqtlo6Ojt7d6xcNDQ1UV1f34fN/8R9+PGwyiYgURSEA5PF41LFYLEayLFM0Gk173uzsLAGgm5sbenh4oLy8POrt7VXzeDxOkiSR3W7/UC+iz5HJ+p05MDAAg8GAhYUFdWxtbQ3t7e0oKChIe15ubi6Afz/e5+fniMfjqK6uVnO9Xg+TyYRAIMBZlx1Wmfn5+RgcHMTBwQEODw8BAMvLy3A4HCnP29raQktLC4xGIyRJgtPpVLNYLAYAmJiYgEajUY+Li4uML23+FPa7ucPhQG5uLhRFwd7eHsxmMywWi5qHQiF0dHTAZDLB7/cjGo1ibm5OzY1GIwBAURQQUcrh8/m467LCvs4sLS1Fd3c33G43rq6uMDU1lZKfnp7i+fkZdrsd5eXlAACNRqPmZrMZOp0Ox8fH3NUyTkbWmaOjo0gkEohEImhra0vJZFkGAOzs7ODx8RHBYBB+v1/NdTod+vv7sb6+jqWlJdzf3yOZTCIcDuP6+joTdfl4x93qXbS2ttLKysqbmcvloqKiIiosLCSbzUaLi4sEgCwWC4VCIXp6eiKXy0WyLFNOTg4ZjUbq7Oyks7Ozd3Xw+XzU1NREJSUlBIAAkMlkIqvVSru7ux96X5+2NPoJfNrS6KeTVTIDgUDKcind0dvb+yX9smrXqKKiIuM7P39CVs3Mvx0hkxEhkxEhkxEhkxEhkxEhkxEhkxEhkxEhkxEhkxEhkxEhkxEhk5G0W3A2m+0ze2QN4XA4bfZqZprNZnR1dWW0UDZTWlqa1o+G/ubd1uxC/BUPJ0ImI0ImI0ImI/8A0px3KNvUgt0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM6D474fYrrt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e237ae18-40cd-4971-f475-d058197649b6"
      },
      "source": [
        "mean, logvar = model.encode(test_sample)\r\n",
        "z = model.reparameterize(mean, logvar)\r\n",
        "predictions = model.sample(z)\r\n",
        "\r\n",
        "z.shape\r\n",
        "logvar.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16, 150)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-19032162293d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mbound_method_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3885\u001b[0m     \u001b[0;31m# However, the replacer is still responsible for attaching self properly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3886\u001b[0m     \u001b[0;31m# TODO(mdan): Is it possible to do it here instead?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3887\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3888\u001b[0m   \u001b[0mweak_bound_method_wrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_method_wrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    <ipython-input-34-5e90344aca9c>:30 sample  *\n        return self.decode(eps, apply_sigmoid=True)\n    <ipython-input-34-5e90344aca9c>:42 decode  *\n        logits = self.decoder(z)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__  **\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py:259 assert_input_compatibility\n        ' but received input with shape ' + display_shape(x.shape))\n\n    ValueError: Input 0 of layer sequential_3 is incompatible with the layer: expected axis -1 of input shape to have value 2 but received input with shape (16, 150)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imMaxZ0-4Ljl"
      },
      "source": [
        "def generate_and_save_images(model, epoch, test_sample):\r\n",
        "    mean, logvar = model.encode(test_sample)\r\n",
        "    z = model.reparameterize(mean, logvar)\r\n",
        "    predictions = model.sample(z)\r\n",
        "    fig = plt.figure(figsize=(4, 4))\r\n",
        "\r\n",
        "    for i in range(predictions.shape[0]):\r\n",
        "        plt.subplot(4, 4, i + 1)\r\n",
        "        plt.imshow(predictions[i, :, :, 0], cmap='gray')\r\n",
        "        plt.axis('off')\r\n",
        "\r\n",
        "    # tight_layout minimizes the overlap between 2 sub-plots\r\n",
        "    plt.savefig('/content/drive/image_at_epoch/image_at_epoch_{:04d}.png'.format(epoch))\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA432zqS5KGw"
      },
      "source": [
        "# Pick a sample of the test set for generating output images\r\n",
        "assert batch_size >= num_examples_to_generate\r\n",
        "for test_batch in test_dataset.take(1):\r\n",
        "    test_sample = test_batch[0:num_examples_to_generate, :, :, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SC5HfRsT6EOg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "443a0a25-1809-4143-d5e4-9c97eee16e47"
      },
      "source": [
        "test_sample.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([16, 28, 28, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZZ2B1pV5ZfF"
      },
      "source": [
        "# generate_and_save_images(model, 0, test_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-PXWYWi5Vl5"
      },
      "source": [
        "generate_and_save_images(model, 0, test_sample)\r\n",
        "\r\n",
        "for epoch in range(1, epochs + 1):\r\n",
        "    start_time = time.time()\r\n",
        "    for train_x in train_dataset:\r\n",
        "        train_step(model, train_x, optimizer)\r\n",
        "    end_time = time.time()\r\n",
        "\r\n",
        "    loss = tf.keras.metrics.Mean()\r\n",
        "    for test_x in test_dataset:\r\n",
        "        loss(compute_loss(model, test_x))\r\n",
        "    elbo = -loss.result()\r\n",
        "    display.clear_output(wait=False)\r\n",
        "    print('Epoch: {}, Test set ELBO: {}, time elapse for current epoch: {}'\r\n",
        "            .format(epoch, elbo, end_time - start_time))\r\n",
        "    generate_and_save_images(model, epoch, test_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPkKBPhwa9td"
      },
      "source": [
        "# Generic Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzqOBlndqu0y"
      },
      "source": [
        "# Uses (z_mean, z_log_var) to sample z, the vector encoding a digit. \r\n",
        "class Sample_Z(tfk.layers.Layer):\r\n",
        "    def call(self, inputs):\r\n",
        "        mu, rho = inputs\r\n",
        "        sd = tf.math.log(1+tf.math.exp(rho))\r\n",
        "        batch_size = tf.shape(mu)[0]\r\n",
        "        dim_z = tf.shape(mu)[1]\r\n",
        "        z_sample = mu + sd * tf.random.normal(shape=(batch_size, dim_z))\r\n",
        "        return z_sample, sd\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjtSyQpToJQL"
      },
      "source": [
        "# Maps MNIST digits to a triplet (z_sample, mu, sd).\r\n",
        "class Encoder(tfkl.Layer):\r\n",
        "\r\n",
        "    def __init__(self,\r\n",
        "                 original_dim=(28,28,1),\r\n",
        "                 latent_dim=40,\r\n",
        "                 hidden_dim=300,\r\n",
        "                 name='encoder',\r\n",
        "                 **kwargs):\r\n",
        "        \r\n",
        "        super(Encoder, self).__init__(name=name, **kwargs)\r\n",
        "        \r\n",
        "        self.original_dim = original_dim\r\n",
        "\r\n",
        "        self.dense_1 = tfkl.Dense(hidden_dim, activation='relu')\r\n",
        "        self.dense_2 = tfkl.Dense(hidden_dim, activation='relu')\r\n",
        "        self.dense_mean = tfkl.Dense(latent_dim, activation=None, name='z_mean')\r\n",
        "        self.dense_raw_stddev  = tfkl.Dense(latent_dim, activation=None, name='z_raw_stddev')\r\n",
        "        self.sampler_z = Sample_Z()\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        x = tfkl.Flatten()(tfkl.Input(shape=self.original_dim))\r\n",
        "        x = self.dense_1(x)\r\n",
        "        x = self.dense_2(x)\r\n",
        "        mu = self.dense_mean(z)\r\n",
        "        rho = self.dense_raw_stddev(z)\r\n",
        "        z_sample, sd = self.sampler_z((mu,rho))\r\n",
        "        return z_sample, mu, sd\r\n",
        "\r\n",
        "# Converts z, the encoded digit vector, back into a readable digit\r\n",
        "class Decoder(tfkl.Layer):\r\n",
        "  \r\n",
        "    def __init__(self,\r\n",
        "                original_dim,\r\n",
        "                hidden_dim=300,\r\n",
        "                name='decoder',\r\n",
        "                **kwargs):\r\n",
        "        super(Decoder, self).__init__(name=name, **kwargs)\r\n",
        "        self.dense_1 = tfkl.Dense(hidden_dim, activation='relu')\r\n",
        "        self.dense_2 = tfkl.Dense(hidden_dim, activation='relu')\r\n",
        "        self.dense_output = tfkl.Dense(original_dim, activation='sigmoid')\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        x = self.dense_1(inputs)\r\n",
        "        x = self.dense_2(x)\r\n",
        "        return self.dense_output(x)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU_SLVZYp0kz"
      },
      "source": [
        "# Combines the encoder and decoder into an end-to-end model for training.\r\n",
        "class VAE(tf.keras.Model):\r\n",
        "\r\n",
        "  def __init__(self,\r\n",
        "               original_dim=(28,28,1),\r\n",
        "               hidden_dim=300,\r\n",
        "               latent_dim=40,\r\n",
        "               learning_rate=0.01,\r\n",
        "               kl_weight=1.0,\r\n",
        "               name='autoencoder',\r\n",
        "               **kwargs):\r\n",
        "    super(VAE, self).__init__(name=name, **kwargs)\r\n",
        "    self.original_dim = original_dim\r\n",
        "    self.learning_rate = learning_rate\r\n",
        "    self.kl_weight = kl_weight\r\n",
        "    self.encoder = Encoder(original_dim=original_dim,\r\n",
        "                           latent_dim=latent_dim,\r\n",
        "                           hidden_dim=hidden_dim)\r\n",
        "    \r\n",
        "    self.decoder = Decoder(original_dim=np.prod(original_dim),\r\n",
        "                           hidden_dim=hidden_dim)\r\n",
        "\r\n",
        "  def call(self, inputs):\r\n",
        "    # self._set_inputs(inputs)\r\n",
        "    z_sample, mu, sd = self.encoder(inputs)\r\n",
        "    reconstructed = self.decoder(z_sample)\r\n",
        "    # Add KL divergence regularization loss.\r\n",
        "    kl_divergence = - 0.5 * tf.math.reduce_sum(1+tf.math.log(\r\n",
        "          tf.math.square(sd))-tf.math.square(mu)-tf.math.square(sd), axis=1)\r\n",
        "    kl_divergence = tf.math.reduce_mean(kl_divergence)\r\n",
        "    # self.add_loss(lambda: self.kl_weight * kl_divergence)\r\n",
        "    self.add_loss(self.kl_weight * kl_divergence)\r\n",
        "    return reconstructed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xbx_GxkAxZaf"
      },
      "source": [
        "# vae loss function -- only the negative log-likelihood part, \r\n",
        "# since we use add_loss for the KL divergence part\r\n",
        "def partial_vae_loss(x_true, model):\r\n",
        "    # x_recons_logits = model.encode_and_decode(x_true)\r\n",
        "    x_recons_logits = model(x_true)\r\n",
        "    # compute cross entropy loss for each dimension of every datapoint\r\n",
        "    raw_cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\r\n",
        "      labels=x_true, logits=x_recons_logits)\r\n",
        "    neg_log_likelihood = tf.math.reduce_sum(raw_cross_entropy, axis=[1, 2, 3])\r\n",
        "    return tf.math.reduce_mean(neg_log_likelihood)\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def train_step(x_true, model, optimizer, loss_metric):\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "        neg_log_lik = partial_vae_loss(x_true, model)\r\n",
        "        # kl_loss = model.losses[-1]\r\n",
        "        kl_loss = tf.math.reduce_sum(model.losses)  # vae.losses is a list\r\n",
        "        total_vae_loss = neg_log_lik + kl_loss\r\n",
        "    gradients = tape.gradient(total_vae_loss, model.trainable_variables)\r\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n",
        "    loss_metric(total_vae_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzaCGmAfxxsz"
      },
      "source": [
        "# set hyperparameters\r\n",
        "train_size = 60000\r\n",
        "batch_size = 100\r\n",
        "test_size = 10000\r\n",
        "latent_dim=40\r\n",
        "input_shape = (28, 28, 1)\r\n",
        "lr = 0.0005\r\n",
        "kl_w = 3\r\n",
        "epochs = 2 #100\r\n",
        "num_examples_to_generate = 16\r\n",
        "\r\n",
        "train_dataset = (tf.data.Dataset.from_tensor_slices(train_images)\r\n",
        "                 .shuffle(train_size).batch(batch_size))\r\n",
        "test_dataset = (tf.data.Dataset.from_tensor_slices(test_images)\r\n",
        "                .shuffle(test_size).batch(batch_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOVjmeODqTZe"
      },
      "source": [
        "# model training\r\n",
        "\r\n",
        "vae = VAE(original_dim=input_shape, hidden_dim=300, latent_dim=40, learning_rate=lr, kl_weight=kl_w)\r\n",
        "loss_metric = tf.keras.metrics.Mean()\r\n",
        "opt = tfk.optimizers.Adam(vae.learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhQRfJ0P1xac"
      },
      "source": [
        "vae.compile(optimizer=opt, loss=loss_metric)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I57PBaRF1Odv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "ea566ad8-e001-43f5-ae79-c7e1de8a6abb"
      },
      "source": [
        "for epoch in tqdm(range(epochs)):\r\n",
        "    start_time = time.time()\r\n",
        "    for train_x in tqdm(train_dataset):\r\n",
        "        train_step(train_x, vae, opt, loss_metric)\r\n",
        "    end_time = time.time()\r\n",
        "    elbo = -loss_metric.result()\r\n",
        "    #display.clear_output(wait=False)\r\n",
        "    print('Epoch: {}, Train set ELBO: {}, time elapse for current epoch: {}'.format(\r\n",
        "            epoch, elbo, end_time - start_time))\r\n",
        "    # generate_images(vae, test_sample)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2 [00:00<?, ?it/s]\n",
            "  0%|          | 0/600 [00:00<?, ?it/s]\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-180-945644156e85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain_x\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_metric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0melbo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mloss_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: in user code:\n\n    <ipython-input-150-f8a3c3c3cbda>:15 train_step  *\n        neg_log_lik = partial_vae_loss(x_true, model)\n    <ipython-input-150-f8a3c3c3cbda>:5 partial_vae_loss  *\n        x_recons_logits = model(x_true)\n    <ipython-input-149-6c3b065fff40>:25 call  *\n        z_sample, mu, sd = self.encoder(inputs)\n    <ipython-input-148-c4040cb87165>:27 call  *\n        z_sample, sd = self.sampler_z((mu,rho))\n    <ipython-input-123-3d2595ddfa68>:7 call  *\n        z_sample = mu + sd * tf.random.normal(shape=(batch_size, dim_z))\n\n    NameError: name 'mu' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzogPx8nXMIx"
      },
      "source": [
        "# TFP Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlpYvtpyX2ta"
      },
      "source": [
        "def custom_sigmoid_cross_entropy_loss_with_logits(x_true, x_recons_logits):\r\n",
        "    raw_cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\r\n",
        "                                                      labels=x_true, \r\n",
        "                                                      logits=x_recons_logits)\r\n",
        "    neg_log_likelihood = tf.math.reduce_sum(raw_cross_entropy, axis=[1, 2, 3])\r\n",
        "    return tf.math.reduce_mean(neg_log_likelihood)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q9jL-NFXhpD"
      },
      "source": [
        "class VAE_MNIST:\r\n",
        "    \r\n",
        "    def __init__(self, dim_z, kl_weight, learning_rate):\r\n",
        "        self.dim_x = (28, 28, 1)\r\n",
        "        self.dim_z = dim_z\r\n",
        "        self.kl_weight = kl_weight\r\n",
        "        self.learning_rate = learning_rate\r\n",
        "\r\n",
        "    # Sequential API encoder\r\n",
        "    def encoder_z(self):\r\n",
        "        # define prior distribution for the code, which is an isotropic Gaussian\r\n",
        "        prior = tfd.Independent(tfd.Normal(loc=tf.zeros(self.dim_z), scale=1.), \r\n",
        "                                reinterpreted_batch_ndims=1)\r\n",
        "        # build layers argument for tfk.Sequential()\r\n",
        "        input_shape = self.dim_x\r\n",
        "        layers = [tfkl.InputLayer(input_shape=input_shape)]\r\n",
        "        layers.append(tfkl.Conv2D(filters=32, kernel_size=3, strides=(2,2), \r\n",
        "                                  padding='valid', activation='relu'))\r\n",
        "        layers.append(tfkl.Conv2D(filters=64, kernel_size=3, strides=(2,2), \r\n",
        "                                  padding='valid', activation='relu'))\r\n",
        "        layers.append(tfkl.Flatten())\r\n",
        "        # the following two lines set the output to be a probabilistic distribution\r\n",
        "        layers.append(tfkl.Dense(tfpl.IndependentNormal.params_size(self.dim_z), \r\n",
        "                                 activation=None, name='z_params'))\r\n",
        "        layers.append(tfpl.IndependentNormal(self.dim_z, \r\n",
        "            convert_to_tensor_fn=tfd.Distribution.sample, \r\n",
        "            activity_regularizer=tfpl.KLDivergenceRegularizer(prior, weight=self.kl_weight), \r\n",
        "            name='z_layer'))\r\n",
        "        return tfk.Sequential(layers, name='encoder')\r\n",
        "    \r\n",
        "    # Sequential API decoder\r\n",
        "    def decoder_x(self):\r\n",
        "        layers = [tfkl.InputLayer(input_shape=self.dim_z)]\r\n",
        "        layers.append(tfkl.Dense(7*7*32, activation=None))\r\n",
        "        layers.append(tfkl.Reshape((7,7,32)))\r\n",
        "        layers.append(tfkl.Conv2DTranspose(filters=64, kernel_size=3, strides=2, \r\n",
        "                                           padding='same', activation='relu'))\r\n",
        "        layers.append(tfkl.Conv2DTranspose(filters=32, kernel_size=3, strides=2, \r\n",
        "                                           padding='same', activation='relu'))\r\n",
        "        layers.append(tfkl.Conv2DTranspose(filters=1, kernel_size=3, strides=1, \r\n",
        "                                           padding='same'))\r\n",
        "        layers.append(tfkl.Flatten(name='x_params'))\r\n",
        "        # note that here we don't need \r\n",
        "        # `tfkl.Dense(tfpl.IndependentBernoulli.params_size(self.dim_x))` because \r\n",
        "        # we've restored the desired input shape with the last Conv2DTranspose layer\r\n",
        "        layers.append(tfpl.IndependentBernoulli(self.dim_x, name='x_layer'))\r\n",
        "        return tfk.Sequential(layers, name='decoder')\r\n",
        "    \r\n",
        "    def build_vae_keras_model(self):\r\n",
        "        x_input = tfk.Input(shape=self.dim_x)\r\n",
        "        encoder = self.encoder_z()\r\n",
        "        decoder = self.decoder_x()\r\n",
        "        z = encoder(x_input)\r\n",
        "\r\n",
        "        # compile VAE model\r\n",
        "        model = tfk.Model(inputs=x_input, outputs=decoder(z))\r\n",
        "        model.compile(loss=custom_sigmoid_cross_entropy_loss_with_logits, \r\n",
        "                      optimizer=tfk.optimizers.Adam(self.learning_rate))\r\n",
        "        return model\r\n",
        "\r\n",
        "# the negative of log-likelihood for probabilistic output\r\n",
        "negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0ms6FUWXzs4"
      },
      "source": [
        "vae = VAE_MNIST(dim_z=40, kl_weight=3, learning_rate=0.0001)\r\n",
        "vae = vae.build_vae_keras_model()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTsuhx_WYbGU",
        "outputId": "0e09df04-5b80-4354-87dc-6e6287abd152",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "with tf.device('/device:GPU:0'):\r\n",
        "    _ = vae.fit(x=train_images, y=train_images, batch_size=batch_size, epochs=15, \r\n",
        "                        verbose=1, validation_data=(test_images, test_images), shuffle=True)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 734.6484 - val_loss: 733.7518\n",
            "Epoch 2/15\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 734.6756 - val_loss: 733.9080\n",
            "Epoch 3/15\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 734.6505 - val_loss: 733.9348\n",
            "Epoch 4/15\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 734.6254 - val_loss: 733.7123\n",
            "Epoch 5/15\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 734.6671 - val_loss: 733.8529\n",
            "Epoch 6/15\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 734.6390 - val_loss: 733.8990\n",
            "Epoch 7/15\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 734.6572 - val_loss: 733.7903\n",
            "Epoch 8/15\n",
            "600/600 [==============================] - 3s 5ms/step - loss: 734.5837 - val_loss: 733.9161\n",
            "Epoch 9/15\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 734.5579 - val_loss: 733.8290\n",
            "Epoch 10/15\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 734.6146 - val_loss: 733.9656\n",
            "Epoch 11/15\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 734.5863 - val_loss: 733.6464\n",
            "Epoch 12/15\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 734.6409 - val_loss: 733.8366\n",
            "Epoch 13/15\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 734.5494 - val_loss: 733.8565\n",
            "Epoch 14/15\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 734.5929 - val_loss: 733.7502\n",
            "Epoch 15/15\n",
            "600/600 [==============================] - 3s 4ms/step - loss: 734.6916 - val_loss: 733.7292\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf4nQb0BYqS0",
        "outputId": "84f66888-063b-4897-a72b-1404cc4aa312",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vae.summary()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "encoder (Sequential)         multiple                  203216    \n",
            "_________________________________________________________________\n",
            "decoder (Sequential)         multiple                  101537    \n",
            "=================================================================\n",
            "Total params: 304,753\n",
            "Trainable params: 304,753\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}